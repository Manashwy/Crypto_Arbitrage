{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly \n",
    "\n",
    "import datetime \n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, data_dir: str):\n",
    "        self.data_dir = data_dir\n",
    "        self.list_of_files = self.get_list_of_files()\n",
    "\n",
    "    def split_filename(self, name):\n",
    "        base = name.replace('.csv', '')\n",
    "        if '_' in base:            return base.split('_')\n",
    "        elif '-' in base:          return base.split('-')\n",
    "        else:\n",
    "            match = re.match(r'([a-z0-9]+?)(usd|usdt|btc|btcf0|gbp|eth|ust|ustf0|eutf0|testusdt|testusdtf0|jpy|eur|xaut|eut|cnht|mxnt|try|mim|xch)$', base)\n",
    "            if match:\n",
    "                return [match.group(1), match.group(2)]\n",
    "            else:\n",
    "                return [base, None]\n",
    "            \n",
    "    def filter_by_currency(self, currency=None):\n",
    "        if currency is None:\n",
    "            return self.list_of_files\n",
    "        if not currency:\n",
    "            return []\n",
    "        if not isinstance(currency, str):\n",
    "            raise ValueError(\"Currency must be a string\")\n",
    "        else:\n",
    "            return [f for f in self.list_of_files if f[0][1] == currency.lower()]\n",
    "        \n",
    "    def get_list_of_files(self):\n",
    "        return [(self.split_filename(f),f) for f in os.listdir(self.data_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    def load_selected(self, selected_files, number_of_files=None, nrows = None, time_indexed=True):\n",
    "        if number_of_files is not None:\n",
    "            selected_files = selected_files[:number_of_files]\n",
    "        if not selected_files:\n",
    "            raise ValueError(\"No files selected for loading.\")\n",
    "        data = {}\n",
    "        for file in selected_files:\n",
    "            file_path = os.path.join(self.data_dir, file[1])\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"File {file_path} does not exist.\")\n",
    "            if nrows is not None:\n",
    "                df = pd.read_csv(file_path, nrows=nrows)\n",
    "            else:\n",
    "                df = pd.read_csv(file_path)\n",
    "            df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
    "            if time_indexed:\n",
    "                df.set_index('time', inplace=True)\n",
    "            # df.loc[:, 'time'] = df['time'].apply(lambda x: datetime.datetime.fromtimestamp(x/1000.0).strftime('%Y-%m-%d %H:%M:%S')).astype('datetime64[ns]')\n",
    "            df = self.validate_and_clean(df)\n",
    "            if df.empty:\n",
    "                print(f\"Warning: DataFrame for {file[0][0]}/{file[0][1]} is empty after loading.\")\n",
    "                continue\n",
    "\n",
    "            data[file[0][0]+\"/\"+file[0][1]] = df\n",
    "        return data\n",
    "    \n",
    "    def validate_and_clean(self, df, freq='1min', max_gap_minutes=10):\n",
    "        print(\"Duplicate index check:\", df.index.duplicated().any())\n",
    "        df = df[~df.index.duplicated(keep='first')]\n",
    "    \n",
    "        print(\"Convert to numeric and reindexing to full range.\")\n",
    "        for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq=freq)\n",
    "        df = df.reindex(full_index)\n",
    "\n",
    "        print(\"Forward filling missing values.\")\n",
    "        missing = df.isnull().any(axis=1)\n",
    "        gap_lengths = missing.astype(int).groupby((~missing).cumsum()).sum()\n",
    "        long_gaps = gap_lengths[gap_lengths > max_gap_minutes]\n",
    "        if not long_gaps.empty:\n",
    "            print(f\"Found long gaps longer than {max_gap_minutes} minutes: {long_gaps.index.tolist()}\")\n",
    "        else:\n",
    "            print(\"No long gaps found.\")\n",
    "        long_gap_indices = gap_lengths[gap_lengths > max_gap_minutes].index\n",
    "        \n",
    "        print(\"Interpolating missing values.\")\n",
    "        df.interpolate(method='linear', limit=max_gap_minutes, inplace=True)\n",
    "        df.ffill(inplace=True)\n",
    "\n",
    "        df['long_gap_flag'] = False\n",
    "        for idx in long_gap_indices:\n",
    "            df.loc[(~missing).cumsum() == idx, 'long_gap_flag'] = True\n",
    "\n",
    "        print(\"Flagging outliers.\")\n",
    "        for col in ['open', 'high', 'low', 'close']:\n",
    "            df[f'{col}_outlier'] = (df[col] <= 0) | (df[col] > df[col].shift(1) * 5)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def synchronize_dataframes(self, df1, df2, how='inner'):\n",
    "        common_index = df1.index.union(df2.index) if how == 'outer' else df1.index.intersection(df2.index)\n",
    "        df1 = df1.reindex(common_index).ffill()\n",
    "        df2 = df2.reindex(common_index).ffill()\n",
    "        return df1, df2\n",
    "\n",
    "    def resample_data(self, df, freq):\n",
    "        resampled = df.resample(freq).agg({\n",
    "            'open': 'first',\n",
    "            'high': 'max',\n",
    "            'low': 'min',\n",
    "            'close': 'last',\n",
    "            'volume': 'sum'\n",
    "        })\n",
    "        return resampled.dropna()\n",
    "\n",
    "    def split_data(self, df, train_frac, val_frac):\n",
    "        assert train_frac + val_frac < 1.0, \"Train + Val fractions must be < 1.0\"\n",
    "        n = len(df)\n",
    "        train_end = int(n * train_frac)\n",
    "        val_end = train_end + int(n * val_frac)\n",
    "\n",
    "        train = df.iloc[:train_end]\n",
    "        val = df.iloc[train_end:val_end]\n",
    "        test = df.iloc[val_end:]\n",
    "\n",
    "        return train, val, test\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate index check: False\n",
      "Convert to numeric and reindexing to full range.\n",
      "Forward filling missing values.\n",
      "Found long gaps longer than 10 minutes: [0]\n",
      "Interpolating missing values.\n",
      "Flagging outliers.\n",
      "Duplicate index check: False\n",
      "Convert to numeric and reindexing to full range.\n",
      "Forward filling missing values.\n",
      "Found long gaps longer than 10 minutes: [526, 919, 954, 993]\n",
      "Interpolating missing values.\n",
      "Flagging outliers.\n",
      "Duplicate index check: False\n",
      "Convert to numeric and reindexing to full range.\n",
      "Forward filling missing values.\n",
      "Found long gaps longer than 10 minutes: [2, 3, 6, 7, 10, 16, 18, 19, 20, 25, 26, 31, 32, 34, 35, 36, 37, 42, 43, 44, 45, 47, 49, 51, 52, 53, 54, 56, 58, 59, 67, 69, 70, 75, 76, 77, 86, 92, 93, 102, 103, 105, 107, 109, 112, 113, 114, 118, 119, 120, 121, 123, 124, 126, 127, 128, 130, 134, 135, 138, 140, 141, 142, 145, 149, 151, 154, 157, 166, 169, 173, 174, 177, 178, 179, 182, 189, 191, 196, 200, 202, 203, 204, 209, 211, 212, 214, 217, 219, 224, 237, 238, 241, 243, 244, 246, 249, 250, 256, 260, 261, 263, 265, 267, 268, 269, 270, 273, 275, 276, 277, 278, 281, 282, 283, 292, 293, 296, 297, 301, 302, 307, 310, 316, 318, 319, 321, 324, 325, 326, 327, 328, 329, 331, 334, 335, 336, 338, 339, 341, 343, 345, 348, 349, 351, 352, 354, 356, 358, 360, 362, 364, 365, 367, 368, 370, 371, 372, 374, 380, 381, 392, 396, 397, 399, 401, 404, 414, 416, 421, 433, 434, 447, 449, 450, 453, 454, 457, 460, 461, 465, 466, 469, 472, 475, 477, 479, 481, 488, 491, 492, 493, 496, 498, 501, 502, 503, 510, 511, 512, 514, 516, 517, 518, 520, 524, 525, 527, 528, 532, 534, 536, 538, 539, 541, 545, 546, 547, 548, 577, 579, 580, 581, 584, 596, 597, 599, 604, 607, 612, 613, 618, 629, 631, 633, 635, 641, 653, 672, 701, 705, 712, 715, 716, 735, 740, 749, 750, 752, 753, 761, 764, 765, 767, 770, 773, 774, 776, 777, 778, 785, 790, 804, 807, 816, 818, 832, 833, 837, 842, 846, 853, 856, 866, 875, 877, 881, 890, 904, 905, 913, 916, 920, 923, 928, 933, 934, 935, 936, 939, 940, 943, 944, 946, 948, 951, 956, 958, 960, 964, 966, 967, 972, 973, 974, 981, 990, 992]\n",
      "Interpolating missing values.\n",
      "Flagging outliers.\n",
      "Duplicate index check: False\n",
      "Convert to numeric and reindexing to full range.\n",
      "Forward filling missing values.\n",
      "Found long gaps longer than 10 minutes: [0]\n",
      "Interpolating missing values.\n",
      "Flagging outliers.\n",
      "Duplicate index check: False\n",
      "Convert to numeric and reindexing to full range.\n",
      "Forward filling missing values.\n",
      "Found long gaps longer than 10 minutes: [3, 6, 11, 26, 27, 30, 40, 44, 45, 47, 48, 49, 53, 54, 67, 68, 69, 72, 73, 84, 90, 92, 100, 112, 116, 117, 120, 121, 123, 136, 140, 143, 145, 148, 149, 167, 176, 183, 195, 202, 208, 212, 224, 225, 228, 230, 232, 239, 242, 248, 249, 255, 259, 261, 262, 263, 266, 274, 284, 291, 293, 299, 307, 317, 329, 334, 358, 362, 366, 373, 378, 379, 384, 394, 405, 409, 416, 423, 425, 426, 431, 443, 450, 469, 471, 472, 473, 474, 507, 520, 523, 528, 536, 548, 550, 551, 553, 556, 585, 594, 595, 596, 598, 599, 606, 609, 614, 616, 621, 624, 625, 626, 628, 629, 639, 640, 646, 651, 652, 676, 683, 688, 689, 692, 695, 703, 709, 710, 718, 719, 720, 722, 724, 728, 729, 730, 731, 735, 736, 739, 741, 745, 748, 751, 755, 759, 762, 764, 776, 778, 781, 783, 786, 792, 796, 798, 800, 807, 809, 812, 814, 817, 826, 839, 840, 842, 844, 849, 862, 870, 871, 874, 877, 885, 886, 891, 893, 898, 900, 901, 902, 906, 914, 916, 919, 920, 922, 925, 929, 930, 931, 933, 934, 936, 939, 940, 941, 945, 948, 949, 952, 953, 958, 968, 969, 971, 972, 973, 974, 976, 977, 979, 980, 982, 983, 984, 991, 995]\n",
      "Interpolating missing values.\n",
      "Flagging outliers.\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/Users/macsnaxx/Personal Projects/Crypto_Arbitrage/crypto_data_2020-2024'\n",
    "\n",
    "datapack = DataManager(data_dir)\n",
    "gab = datapack.filter_by_currency('usd')\n",
    "gab = datapack.load_selected(gab, 5, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class FileReader:\n",
    "    def __init__(self, data_dir: str):\n",
    "        self.data_dir = data_dir\n",
    "        self.list_of_files = self.get_list_of_files()\n",
    "        self.number_of_files = len(self.list_of_files)\n",
    "        \n",
    "    def get_list_of_files(self):\n",
    "        return [(self.split_filename(f), f) for f in os.listdir(self.data_dir) if f.endswith('.csv')]\n",
    "\n",
    "\n",
    "    def split_filename(self, name):\n",
    "        base = name.replace('.csv', '')\n",
    "        if '_' in base:\n",
    "            return base.split('_')\n",
    "        elif '-' in base:\n",
    "            return base.split('-')\n",
    "        else:\n",
    "            match = re.match(r'([a-z0-9]+?)(usd|usdt|btc|btcf0|gbp|eth|ust|ustf0|eutf0|testusdt|testusdtf0|jpy|eur|xaut|eut|cnht|mxnt|try|mim|xch)$', base)\n",
    "            return [match.group(1), match.group(2)] if match else [base, None]\n",
    "\n",
    "\n",
    "    def filter_by_currency(self, currency=None):\n",
    "        if currency is None:\n",
    "            return self.list_of_files\n",
    "        if not isinstance(currency, str):\n",
    "            raise ValueError(\"Currency must be a string\")\n",
    "        return [f for f in self.list_of_files if f[0][1] == currency.lower()]\n",
    "\n",
    "    def load_latest_data(self, selected_files, number_of_files=None, nrows=None, min_rows=105040):\n",
    "        if number_of_files is not None:\n",
    "            selected_files = selected_files[:number_of_files]\n",
    "        if not selected_files:\n",
    "            raise ValueError(\"No files selected for loading.\")\n",
    "\n",
    "        data = {}\n",
    "        for file in selected_files:\n",
    "            path = os.path.join(self.data_dir, file[1])\n",
    "            if not os.path.exists(path):\n",
    "                continue\n",
    "\n",
    "            df = pd.read_csv(path)\n",
    "            if len(df) < min_rows:\n",
    "                print(f\"Skipping {file[1]}: not enough data.\")\n",
    "                continue\n",
    "            df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
    "            df.set_index('time', inplace=True)\n",
    "            if nrows is not None:\n",
    "                df = df.tail(nrows)\n",
    "            df.sort_index(inplace=True)\n",
    "            data[file[0][0] + \"/\" + file[0][1]] = df\n",
    "\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping tonusd.csv: not enough data.\n",
      "Skipping galausd.csv: not enough data.\n",
      "Skipping velo-usd.csv: not enough data.\n",
      "Skipping waxusd.csv: not enough data.\n",
      "Skipping planetsusd.csv: not enough data.\n",
      "Skipping boousd.csv: not enough data.\n",
      "Skipping astusd.csv: not enough data.\n",
      "Skipping briseusd.csv: not enough data.\n",
      "Skipping exousd.csv: not enough data.\n",
      "Skipping suku-usd.csv: not enough data.\n",
      "Skipping band-usd.csv: not enough data.\n",
      "Skipping chzusd.csv: not enough data.\n",
      "Skipping boson-usd.csv: not enough data.\n",
      "Skipping arbusd.csv: not enough data.\n",
      "Skipping testalgotestusd.csv: not enough data.\n",
      "Skipping xautusd.csv: not enough data.\n",
      "Skipping genusd.csv: not enough data.\n",
      "Skipping reefusd.csv: not enough data.\n",
      "Skipping luxousd.csv: not enough data.\n",
      "Skipping pngusd.csv: not enough data.\n",
      "Skipping gmtusd.csv: not enough data.\n",
      "Skipping laiusd.csv: not enough data.\n",
      "Skipping sidus_usd.csv: not enough data.\n",
      "Skipping dora_usd.csv: not enough data.\n",
      "Skipping swmusd.csv: not enough data.\n",
      "Skipping polisusd.csv: not enough data.\n",
      "Skipping gxtusd.csv: not enough data.\n",
      "Skipping maticusd.csv: not enough data.\n",
      "Skipping dogeusd.csv: not enough data.\n",
      "Skipping chex-usd.csv: not enough data.\n",
      "Skipping wncg_usd.csv: not enough data.\n",
      "Skipping flokiusd.csv: not enough data.\n",
      "Skipping testltctestusd.csv: not enough data.\n",
      "Skipping gala_usd.csv: not enough data.\n",
      "Skipping matic_usd.csv: not enough data.\n",
      "Skipping spellusd.csv: not enough data.\n",
      "Skipping suiusd.csv: not enough data.\n",
      "Skipping oneusd.csv: not enough data.\n",
      "Skipping egldusd.csv: not enough data.\n",
      "Skipping reef-usd.csv: not enough data.\n",
      "Skipping zbtusd.csv: not enough data.\n",
      "Skipping waves_usd.csv: not enough data.\n",
      "Skipping ocean-usd.csv: not enough data.\n",
      "Skipping okbusd.csv: not enough data.\n",
      "Skipping antusd.csv: not enough data.\n",
      "Skipping 1inch-usd.csv: not enough data.\n",
      "Skipping woousd.csv: not enough data.\n",
      "Skipping luna_usd.csv: not enough data.\n",
      "Skipping testxtztestusd.csv: not enough data.\n",
      "Skipping senateusd.csv: not enough data.\n",
      "Skipping planets-usd.csv: not enough data.\n",
      "Skipping polc_usd.csv: not enough data.\n",
      "Skipping ctxusd.csv: not enough data.\n",
      "Skipping gptusd.csv: not enough data.\n",
      "Skipping boba_usd.csv: not enough data.\n",
      "Skipping bchabc-usd.csv: not enough data.\n",
      "Skipping orsusd.csv: not enough data.\n",
      "Skipping sand_usd.csv: not enough data.\n",
      "Skipping veeusd.csv: not enough data.\n",
      "Skipping bchnusd.csv: not enough data.\n",
      "Skipping necusd.csv: not enough data.\n",
      "Skipping qtfusd.csv: not enough data.\n",
      "Skipping chsb-usd.csv: not enough data.\n",
      "Skipping htxusd.csv: not enough data.\n",
      "Skipping vrausd.csv: not enough data.\n",
      "Skipping shftusd.csv: not enough data.\n",
      "Skipping bmiusd.csv: not enough data.\n",
      "Skipping luxo_usd.csv: not enough data.\n",
      "Skipping bchabcusd.csv: not enough data.\n",
      "Skipping spell_usd.csv: not enough data.\n",
      "Skipping thetausd.csv: not enough data.\n",
      "Skipping qrdo_usd.csv: not enough data.\n",
      "Skipping testavaxtestusd.csv: not enough data.\n",
      "Skipping tlos_usd.csv: not enough data.\n",
      "Skipping sxxusd.csv: not enough data.\n",
      "Skipping ocean_usd.csv: not enough data.\n",
      "Skipping waves-usd.csv: not enough data.\n",
      "Skipping reef_usd.csv: not enough data.\n",
      "Skipping testmatictestusd.csv: not enough data.\n",
      "Skipping rrbusd.csv: not enough data.\n",
      "Skipping nomusd.csv: not enough data.\n",
      "Skipping planets_usd.csv: not enough data.\n",
      "Skipping velousd.csv: not enough data.\n",
      "Skipping tenet_usd.csv: not enough data.\n",
      "Skipping luna-usd.csv: not enough data.\n",
      "Skipping testdogetestusd.csv: not enough data.\n",
      "Skipping shib_usd.csv: not enough data.\n",
      "Skipping dtxusd.csv: not enough data.\n",
      "Skipping 1inch_usd.csv: not enough data.\n",
      "Skipping tlosusd.csv: not enough data.\n",
      "Skipping chsb_usd.csv: not enough data.\n",
      "Skipping wminimausd.csv: not enough data.\n",
      "Skipping ethwusd.csv: not enough data.\n",
      "Skipping eosdt-usd.csv: not enough data.\n",
      "Skipping grtusd.csv: not enough data.\n",
      "Skipping bchabc_usd.csv: not enough data.\n",
      "Skipping senate_usd.csv: not enough data.\n",
      "Skipping uopusd.csv: not enough data.\n",
      "Skipping hecusd.csv: not enough data.\n",
      "Skipping sushiusd.csv: not enough data.\n",
      "Skipping zilusd.csv: not enough data.\n",
      "Skipping odeusd.csv: not enough data.\n",
      "Skipping prmx_usd.csv: not enough data.\n",
      "Skipping suku_usd.csv: not enough data.\n",
      "Skipping band_usd.csv: not enough data.\n",
      "Skipping testethtestusd.csv: not enough data.\n",
      "Skipping nexousd.csv: not enough data.\n",
      "Skipping wminima_usd.csv: not enough data.\n",
      "Skipping testapttestusd.csv: not enough data.\n",
      "Skipping velo_usd.csv: not enough data.\n",
      "Skipping gstusd.csv: not enough data.\n",
      "Skipping sidususd.csv: not enough data.\n",
      "Skipping eth2xusd.csv: not enough data.\n",
      "Skipping prmxusd.csv: not enough data.\n",
      "Skipping boson_usd.csv: not enough data.\n",
      "Skipping testxauttestusd.csv: not enough data.\n",
      "Skipping nutusd.csv: not enough data.\n",
      "Skipping dcrusd.csv: not enough data.\n",
      "Skipping chex_usd.csv: not enough data.\n",
      "Skipping kaiusd.csv: not enough data.\n",
      "Skipping lunausd.csv: not enough data.\n",
      "Skipping albt-usd.csv: not enough data.\n",
      "Skipping polcusd.csv: not enough data.\n",
      "Skipping iosusd.csv: not enough data.\n",
      "Skipping bandusd.csv: not enough data.\n",
      "Skipping gotusd.csv: not enough data.\n",
      "Skipping pepe_usd.csv: not enough data.\n",
      "Skipping nearusd.csv: not enough data.\n",
      "Skipping poausd.csv: not enough data.\n",
      "Skipping testadatestusd.csv: not enough data.\n",
      "Skipping flrusd.csv: not enough data.\n",
      "Skipping dvfusd.csv: not enough data.\n",
      "Skipping dorausd.csv: not enough data.\n",
      "Skipping near-usd.csv: not enough data.\n",
      "Skipping aave-usd.csv: not enough data.\n",
      "Skipping sushi-usd.csv: not enough data.\n",
      "Skipping forthusd.csv: not enough data.\n",
      "Skipping terraustusd.csv: not enough data.\n",
      "Skipping chexusd.csv: not enough data.\n",
      "Skipping forth-usd.csv: not enough data.\n",
      "Skipping egld_usd.csv: not enough data.\n",
      "Skipping luna2usd.csv: not enough data.\n",
      "Skipping apenft_usd.csv: not enough data.\n",
      "Skipping testdottestusd.csv: not enough data.\n",
      "Skipping karate_usd.csv: not enough data.\n",
      "Skipping ringx-usd.csv: not enough data.\n",
      "Skipping xaut-usd.csv: not enough data.\n",
      "Skipping testneartestusd.csv: not enough data.\n",
      "Skipping eth2x_usd.csv: not enough data.\n",
      "Skipping rcnusd.csv: not enough data.\n",
      "Skipping omnusd.csv: not enough data.\n",
      "Skipping oceanusd.csv: not enough data.\n",
      "Skipping opxusd.csv: not enough data.\n",
      "Skipping trade_usd.csv: not enough data.\n",
      "Skipping bntusd.csv: not enough data.\n",
      "Skipping doge-usd.csv: not enough data.\n",
      "Skipping wprusd.csv: not enough data.\n",
      "Skipping xcnusd.csv: not enough data.\n",
      "Skipping whbt_usd.csv: not enough data.\n",
      "Skipping theta_usd.csv: not enough data.\n",
      "Skipping mxnt_usd.csv: not enough data.\n",
      "Skipping b21x-usd.csv: not enough data.\n",
      "Skipping nexo_usd.csv: not enough data.\n",
      "Skipping qrdousd.csv: not enough data.\n",
      "Skipping nxrausd.csv: not enough data.\n",
      "Skipping comp_usd.csv: not enough data.\n",
      "Skipping pasusd.csv: not enough data.\n",
      "Skipping sngusd.csv: not enough data.\n",
      "Skipping sweatusd.csv: not enough data.\n",
      "Skipping wtcusd.csv: not enough data.\n",
      "Skipping dusk-usd.csv: not enough data.\n",
      "Skipping enjusd.csv: not enough data.\n",
      "Skipping boxusd.csv: not enough data.\n",
      "Skipping yggusd.csv: not enough data.\n",
      "Skipping convusd.csv: not enough data.\n",
      "Skipping linkusd.csv: not enough data.\n",
      "Skipping rifusd.csv: not enough data.\n",
      "Skipping bobausd.csv: not enough data.\n",
      "Skipping compusd.csv: not enough data.\n",
      "Skipping treeb_usd.csv: not enough data.\n",
      "Skipping ncausd.csv: not enough data.\n",
      "Skipping exrd-usd.csv: not enough data.\n",
      "Skipping testbtctestusd.csv: not enough data.\n",
      "Skipping bchn-usd.csv: not enough data.\n",
      "Skipping wavesusd.csv: not enough data.\n",
      "Skipping xcad_usd.csv: not enough data.\n",
      "Skipping link-usd.csv: not enough data.\n",
      "Skipping ldousd.csv: not enough data.\n",
      "Skipping hotusd.csv: not enough data.\n",
      "Skipping blurusd.csv: not enough data.\n",
      "Skipping atlasusd.csv: not enough data.\n",
      "Skipping best_usd.csv: not enough data.\n",
      "Skipping btse-usd.csv: not enough data.\n",
      "Skipping eususd.csv: not enough data.\n",
      "Skipping atlas_usd.csv: not enough data.\n",
      "Skipping avax-usd.csv: not enough data.\n",
      "Skipping shibusd.csv: not enough data.\n",
      "Skipping bgbusd.csv: not enough data.\n",
      "Skipping wildusd.csv: not enough data.\n",
      "Skipping hezusd.csv: not enough data.\n",
      "Skipping 1inchusd.csv: not enough data.\n",
      "Skipping terraust-usd.csv: not enough data.\n",
      "Skipping bosonusd.csv: not enough data.\n",
      "Skipping dusk_usd.csv: not enough data.\n",
      "Skipping luna2_usd.csv: not enough data.\n",
      "Skipping blur_usd.csv: not enough data.\n",
      "Skipping bchn_usd.csv: not enough data.\n",
      "Skipping wncgusd.csv: not enough data.\n",
      "Skipping tknusd.csv: not enough data.\n",
      "Skipping ognusd.csv: not enough data.\n",
      "Skipping treebusd.csv: not enough data.\n",
      "Skipping polis_usd.csv: not enough data.\n",
      "Skipping best-usd.csv: not enough data.\n",
      "Skipping dapp-usd.csv: not enough data.\n",
      "Skipping btse_usd.csv: not enough data.\n",
      "Skipping gocusd.csv: not enough data.\n",
      "Skipping mxntusd.csv: not enough data.\n",
      "Skipping manusd.csv: not enough data.\n",
      "Skipping testsoltestusd.csv: not enough data.\n",
      "Skipping mtnusd.csv: not enough data.\n",
      "Skipping link_usd.csv: not enough data.\n",
      "Skipping avax_usd.csv: not enough data.\n",
      "Skipping dtausd.csv: not enough data.\n",
      "Skipping shft_usd.csv: not enough data.\n",
      "Skipping drnusd.csv: not enough data.\n",
      "Skipping sandusd.csv: not enough data.\n",
      "Skipping testeostestusd.csv: not enough data.\n",
      "Skipping ethw_usd.csv: not enough data.\n",
      "Skipping bestusd.csv: not enough data.\n",
      "Skipping btseusd.csv: not enough data.\n",
      "Skipping forth_usd.csv: not enough data.\n",
      "Skipping egld-usd.csv: not enough data.\n",
      "Skipping seiusd.csv: not enough data.\n",
      "Skipping apenftusd.csv: not enough data.\n",
      "Skipping triusd.csv: not enough data.\n",
      "Skipping b2musd.csv: not enough data.\n",
      "Skipping sushi_usd.csv: not enough data.\n",
      "Skipping aave_usd.csv: not enough data.\n",
      "Skipping wild_usd.csv: not enough data.\n",
      "Skipping dgxusd.csv: not enough data.\n",
      "Skipping near_usd.csv: not enough data.\n",
      "Skipping uskusd.csv: not enough data.\n",
      "Skipping brise_usd.csv: not enough data.\n",
      "Skipping eth2x-usd.csv: not enough data.\n",
      "Skipping xaut_usd.csv: not enough data.\n",
      "Skipping chsbusd.csv: not enough data.\n",
      "Skipping tradeusd.csv: not enough data.\n",
      "Skipping requsd.csv: not enough data.\n",
      "Skipping srmusd.csv: not enough data.\n",
      "Skipping onlusd.csv: not enough data.\n",
      "Skipping sweat_usd.csv: not enough data.\n",
      "Skipping xcadusd.csv: not enough data.\n",
      "Skipping jasmy_usd.csv: not enough data.\n",
      "Skipping doge_usd.csv: not enough data.\n",
      "Skipping cndusd.csv: not enough data.\n",
      "Skipping testfiltestusd.csv: not enough data.\n",
      "Skipping funusd.csv: not enough data.\n",
      "Skipping sukuusd.csv: not enough data.\n",
      "Skipping nexo-usd.csv: not enough data.\n",
      "Skipping jasmyusd.csv: not enough data.\n",
      "Skipping comp-usd.csv: not enough data.\n",
      "Skipping fttusd.csv: not enough data.\n",
      "Skipping bmnusd.csv: not enough data.\n",
      "Skipping aaveusd.csv: not enough data.\n",
      "Skipping duskusd.csv: not enough data.\n",
      "Skipping floki_usd.csv: not enough data.\n",
      "Skipping nxra_usd.csv: not enough data.\n",
      "Skipping avtusd.csv: not enough data.\n",
      "Skipping avaxusd.csv: not enough data.\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/Users/macsnaxx/Personal Projects/Crypto_Arbitrage/crypto_data_2020-2024'\n",
    "\n",
    "files = FileReader(data_dir)\n",
    "selc = files.filter_by_currency('usd')\n",
    "raw_dataset = files.load_latest_data(selc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        # self.close_data = self.create_close_data(data)\n",
    "\n",
    "    def get_dataframe_in_date_range(self, start, end, freq, pairs=None, margin = 50):\n",
    "        return_data = {}\n",
    "        expected = pd.date_range(start, end, freq=freq)\n",
    "        for pair, df in self.data.items():\n",
    "            if pairs is not None and pair not in pairs:\n",
    "                continue\n",
    "            actual = df.reindex(expected)[start:end].resample(freq).last()\n",
    "            fre  = round((actual.isna().sum().max()/len(actual))*100, 2)\n",
    "            if fre > margin:\n",
    "                continue\n",
    "            print(\"Getting pair - \", pair)\n",
    "            print(\"Expected - \", len(actual), \" | Actual - \", len(actual.dropna()))\n",
    "            print('Missing in range: ',fre, \"%\")\n",
    "            return_data[pair] = actual.dropna(how = 'all')\n",
    "            print(\"---\")\n",
    "        return return_data\n",
    "\n",
    "\n",
    "    def validate_and_clean(self, df, max_gap_minutes=10):\n",
    "        print(\"Duplicate index check:\", df.index.duplicated().any())\n",
    "        df = df[~df.index.duplicated(keep='first')]\n",
    "    \n",
    "        print(\"Convert to numeric and reindexing to full range.\")\n",
    "        for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq=freq)\n",
    "        df = df.reindex(full_index)\n",
    "\n",
    "        print(\"Forward filling missing values.\")\n",
    "        missing = df.isnull().any(axis=1)\n",
    "        gap_lengths = missing.astype(int).groupby((~missing).cumsum()).sum()\n",
    "        long_gaps = gap_lengths[gap_lengths > max_gap_minutes]\n",
    "        if not long_gaps.empty:\n",
    "            print(f\"Found long gaps longer than {max_gap_minutes} minutes: {long_gaps.index.tolist()}\")\n",
    "        else:\n",
    "            print(\"No long gaps found.\")\n",
    "        long_gap_indices = gap_lengths[gap_lengths > max_gap_minutes].index\n",
    "        \n",
    "        print(\"Interpolating missing values.\")\n",
    "        df.interpolate(method='linear', limit=max_gap_minutes, inplace=True)\n",
    "        df.ffill(inplace=True)\n",
    "\n",
    "        df['long_gap_flag'] = False\n",
    "        for idx in long_gap_indices:\n",
    "            df.loc[(~missing).cumsum() == idx, 'long_gap_flag'] = True\n",
    "\n",
    "        print(\"Flagging outliers.\")\n",
    "        for col in ['open', 'high', 'low', 'close']:\n",
    "            df[f'{col}_outlier'] = (df[col] <= 0) | (df[col] > df[col].shift(1) * 5)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def synchronize_dataframes(self, df1, df2, how='inner'):\n",
    "        common_index = df1.index.union(df2.index) if how == 'outer' else df1.index.intersection(df2.index)\n",
    "        df1 = df1.reindex(common_index).ffill()\n",
    "        df2 = df2.reindex(common_index).ffill()\n",
    "        return df1, df2\n",
    "\n",
    "    def resample_data(self, df, freq):\n",
    "        resampled = df.resample(freq).agg({\n",
    "            'open': 'first',\n",
    "            'high': 'max',\n",
    "            'low': 'min',\n",
    "            'close': 'last',\n",
    "            'volume': 'sum'\n",
    "        })\n",
    "        return resampled.dropna()\n",
    "\n",
    "    def split_data(self, df, train_frac, val_frac):\n",
    "        assert train_frac + val_frac < 1.0, \"Train + Val fractions must be < 1.0\"\n",
    "        n = len(df)\n",
    "        train_end = int(n * train_frac)\n",
    "        val_end = train_end + int(n * val_frac)\n",
    "\n",
    "        train = df.iloc[:train_end]\n",
    "        val = df.iloc[train_end:val_end]\n",
    "        test = df.iloc[val_end:]\n",
    "\n",
    "        return train, val, test\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def prepare_for_model():\n",
    "        pass \n",
    "\n",
    "    def create_close_data(self, data):\n",
    "        close_data = {}\n",
    "        for pair, df in data.items():\n",
    "            if 'close' in df.columns:\n",
    "                close_data[pair] = df['close']\n",
    "            else:\n",
    "                print(f\"Warning: 'close' column not found in {pair}. Skipping.\")\n",
    "        return pd.DataFrame(close_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataPreprocessor(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting pair -  xrp/usd\n",
      "Expected -  8761  | Actual -  8655\n",
      "Missing in range:  1.21 %\n",
      "---\n",
      "Getting pair -  eos/usd\n",
      "Expected -  8761  | Actual -  7266\n",
      "Missing in range:  17.06 %\n",
      "---\n",
      "Getting pair -  zec/usd\n",
      "Expected -  8761  | Actual -  5739\n",
      "Missing in range:  34.49 %\n",
      "---\n",
      "Getting pair -  trx/usd\n",
      "Expected -  8761  | Actual -  6909\n",
      "Missing in range:  21.14 %\n",
      "---\n",
      "Getting pair -  ust/usd\n",
      "Expected -  8761  | Actual -  8019\n",
      "Missing in range:  8.47 %\n",
      "---\n",
      "Getting pair -  eth/usd\n",
      "Expected -  8761  | Actual -  8725\n",
      "Missing in range:  0.41 %\n",
      "---\n",
      "Getting pair -  neo/usd\n",
      "Expected -  8761  | Actual -  5771\n",
      "Missing in range:  34.13 %\n",
      "---\n",
      "Getting pair -  ltc/usd\n",
      "Expected -  8761  | Actual -  8393\n",
      "Missing in range:  4.2 %\n",
      "---\n",
      "Getting pair -  xmr/usd\n",
      "Expected -  8761  | Actual -  4929\n",
      "Missing in range:  43.74 %\n",
      "---\n",
      "Getting pair -  omg/usd\n",
      "Expected -  8761  | Actual -  5739\n",
      "Missing in range:  34.49 %\n",
      "---\n",
      "Getting pair -  bsv/usd\n",
      "Expected -  8761  | Actual -  4627\n",
      "Missing in range:  47.19 %\n",
      "---\n",
      "Getting pair -  xlm/usd\n",
      "Expected -  8761  | Actual -  5972\n",
      "Missing in range:  31.83 %\n",
      "---\n",
      "Getting pair -  btc/usd\n",
      "Expected -  8761  | Actual -  8733\n",
      "Missing in range:  0.32 %\n",
      "---\n",
      "Getting pair -  iot/usd\n",
      "Expected -  8761  | Actual -  5809\n",
      "Missing in range:  33.69 %\n",
      "---\n",
      "Getting pair -  dsh/usd\n",
      "Expected -  8761  | Actual -  6203\n",
      "Missing in range:  29.2 %\n",
      "---\n",
      "Getting pair -  zrx/usd\n",
      "Expected -  8761  | Actual -  4432\n",
      "Missing in range:  49.41 %\n",
      "---\n",
      "Getting pair -  etc/usd\n",
      "Expected -  8761  | Actual -  4999\n",
      "Missing in range:  42.94 %\n",
      "---\n",
      "Getting pair -  xtz/usd\n",
      "Expected -  8761  | Actual -  5356\n",
      "Missing in range:  38.87 %\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "st = pd.Timestamp('2021-01-01')\n",
    "ed = pd.Timestamp('2022-01-01')\n",
    "fred = data.get_dataframe_in_date_range(st, ed, '1h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = data.create_close_data(fred)\n",
    "# all_data = (all_data - all_data.mean())/all_data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_1</th>\n",
       "      <th>pair_2</th>\n",
       "      <th>score</th>\n",
       "      <th>p_value</th>\n",
       "      <th>cointegrated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neo</td>\n",
       "      <td>ltc</td>\n",
       "      <td>-18.085699</td>\n",
       "      <td>1.754755e-29</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neo</td>\n",
       "      <td>btc</td>\n",
       "      <td>-18.031197</td>\n",
       "      <td>1.825176e-29</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neo</td>\n",
       "      <td>dsh</td>\n",
       "      <td>-17.899075</td>\n",
       "      <td>2.030105e-29</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neo</td>\n",
       "      <td>omg</td>\n",
       "      <td>-17.882449</td>\n",
       "      <td>2.059746e-29</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>etc</td>\n",
       "      <td>xtz</td>\n",
       "      <td>-17.472872</td>\n",
       "      <td>3.181394e-29</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>ust</td>\n",
       "      <td>dsh</td>\n",
       "      <td>-0.794067</td>\n",
       "      <td>9.354790e-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>ust</td>\n",
       "      <td>xtz</td>\n",
       "      <td>-0.753544</td>\n",
       "      <td>9.405412e-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>ust</td>\n",
       "      <td>omg</td>\n",
       "      <td>-0.753438</td>\n",
       "      <td>9.405538e-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>ust</td>\n",
       "      <td>neo</td>\n",
       "      <td>-0.682181</td>\n",
       "      <td>9.485271e-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>ust</td>\n",
       "      <td>etc</td>\n",
       "      <td>-0.668751</td>\n",
       "      <td>9.499061e-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    pair_1 pair_2      score       p_value  cointegrated\n",
       "0      neo    ltc -18.085699  1.754755e-29          True\n",
       "1      neo    btc -18.031197  1.825176e-29          True\n",
       "2      neo    dsh -17.899075  2.030105e-29          True\n",
       "3      neo    omg -17.882449  2.059746e-29          True\n",
       "4      etc    xtz -17.472872  3.181394e-29          True\n",
       "..     ...    ...        ...           ...           ...\n",
       "100    ust    dsh  -0.794067  9.354790e-01         False\n",
       "101    ust    xtz  -0.753544  9.405412e-01         False\n",
       "102    ust    omg  -0.753438  9.405538e-01         False\n",
       "103    ust    neo  -0.682181  9.485271e-01         False\n",
       "104    ust    etc  -0.668751  9.499061e-01         False\n",
       "\n",
       "[105 rows x 5 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.tsa.stattools import coint\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "def test_cointegration_and_rank(base_data, significance_level=0.05):\n",
    "    results = []\n",
    "\n",
    "    for pair1, pair2 in combinations(base_data.columns,2):\n",
    "        try:\n",
    "            \n",
    "            x = base_data[pair1]\n",
    "            y =  base_data[pair2]\n",
    "\n",
    "            if len(x.dropna()) < 30 or len(y.dropna()) < 30:\n",
    "                continue\n",
    "\n",
    "            score, pvalue, _ = coint(x, y)\n",
    "        \n",
    "\n",
    "            results.append({\n",
    "                'pair_1': pair1.rstrip(\"/usd\"),\n",
    "                'pair_2': pair2.rstrip(\"/usd\"),\n",
    "                'score': score,\n",
    "                'p_value': pvalue,\n",
    "                'cointegrated': pvalue < significance_level\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error testing {pair1} vs {pair2}: {e}\")\n",
    "            continue\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values(by='p_value').reset_index(drop=True)\n",
    "    return results_df\n",
    "\n",
    "st = pd.Timestamp('2021-05-10 09:00:00')\n",
    "ed = pd.Timestamp('2021-05-11 09:00:00')\n",
    "\n",
    "test_cointegration_and_rank(all_data[st:ed].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def validate_and_clean(self, df, freq='1min', max_gap=100):\n",
    "        print(\"Validating and cleaning data...\")\n",
    "        print(\"Duplicate index check:\", df.index.duplicated().any())\n",
    "        df = df[~df.index.duplicated(keep='first')]\n",
    "        df = df[['open', 'high', 'low', 'close', 'volume']].copy()\n",
    "\n",
    "        print(\"Convert to numeric and reindexing to full range.\")\n",
    "        for col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq=freq)\n",
    "        df = df.reindex(full_index)\n",
    "\n",
    "        # print(\"Forward filling missing values.\")\n",
    "        # missing = df.isnull().any(axis=1)\n",
    "        # print(\"Missing values detected:\", missing.sum())\n",
    "        # gap_lengths = missing.astype(int).groupby((~missing).cumsum()).sum()\n",
    "        # print(\"Maximum gap length:\", max(gap_lengths), \"minutes\")\n",
    "        # print(\"Allowable gaps:\", max_gap, \"minutes\")\n",
    "        # print(\"Gaps longer than max_gap:\", gap_lengths[gap_lengths > max_gap].index)\n",
    "        # if any(gap_lengths > max_gap):\n",
    "        #     return None  \n",
    "\n",
    "        # df.interpolate(method='linear', limit=max_gap, inplace=True)\n",
    "        # df.ffill(inplace=True)\n",
    "\n",
    "        print(\"Flagging outliers.\")\n",
    "        for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "            df[f'{col}_outlier'] = (df[col] <= 0)\n",
    "            df[f'{col}_outlier'] |= ((df[col] > df[col].shift(1) * 5) | (df[col] < df[col].shift(1) / 5))\n",
    "            returns = df[col].pct_change()\n",
    "            z_scores = (returns - returns.rolling(24).mean()) / returns.rolling(24).std()\n",
    "            df[f'{col}_outlier'] |= (np.abs(z_scores) > 5)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def resample_data(self, df, freq='5min'):\n",
    "        return df.resample(freq).agg({\n",
    "            'open': 'first',\n",
    "            'high': 'max',\n",
    "            'low': 'min',\n",
    "            'close': 'last',\n",
    "            'volume': 'sum'\n",
    "        }).dropna()\n",
    "\n",
    "    def split_data(self, df, train_frac=0.6, val_frac=0.2):\n",
    "        assert train_frac + val_frac < 1.0\n",
    "        n = len(df)\n",
    "        train_end = int(n * train_frac)\n",
    "        val_end = train_end + int(n * val_frac)\n",
    "        return df.iloc[:train_end], df.iloc[train_end:val_end], df.iloc[val_end:]\n",
    "    \n",
    "    def check_missing_ohlcv(self,\n",
    "        df: pd.DataFrame,\n",
    "        start_time: pd.Timestamp,\n",
    "        end_time: pd.Timestamp,\n",
    "        freq: str,\n",
    "        pair_name: str,\n",
    "    ):\n",
    "\n",
    "        df = df.copy()\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            if 'timestamp' in df.columns:\n",
    "                df.index = pd.to_datetime(df['timestamp'])\n",
    "            else:\n",
    "                raise ValueError(\"DataFrame must have a DatetimeIndex or a 'timestamp' column\")\n",
    "\n",
    "        # 2. Define what timestamps we *should* have\n",
    "        expected_index = pd.date_range(start=start_time, end=end_time, freq=freq)\n",
    "        expected_count = len(expected_index)\n",
    "\n",
    "        # 3. Restrict to the requested window\n",
    "        windowed = df.loc[start_time:end_time]\n",
    "\n",
    "        # 4. Reindex to “expose” any completely missing rows\n",
    "        reindexed = windowed.reindex(expected_index)\n",
    "\n",
    "        # 5. Count row‐level gaps (all‐NaN rows == missing timestamp)\n",
    "        missing_mask = reindexed.isnull().all(axis=1)\n",
    "        missing_timestamps = missing_mask.sum()\n",
    "        missing_list = reindexed[missing_mask].index.to_list()\n",
    "\n",
    "        # 6. Count column‐wise nulls (for partial gaps)\n",
    "        missing_by_column = reindexed.isnull().sum().to_dict()\n",
    "\n",
    "        # 7. Pack up\n",
    "        result = {\n",
    "            'pair': pair_name,\n",
    "            'expected_timestamps': expected_count,\n",
    "            'actual_timestamps': expected_count - missing_timestamps,\n",
    "            'missing_timestamps': missing_timestamps,\n",
    "            'missing_timestamps_list': missing_list,\n",
    "            'missing_by_column': missing_by_column,\n",
    "        }\n",
    "\n",
    "        # 8. (Optional) print a quick summary\n",
    "        print(f\"[{pair_name}] Expected: {expected_count:,} rows, \"\n",
    "            f\"Found: {result['actual_timestamps']:,}, \"\n",
    "            f\"Missing rows: {missing_timestamps:,}\")\n",
    "        for col, n in missing_by_column.items():\n",
    "            if n:\n",
    "                print(f\"  – {col!r}: {n:,} nulls\")\n",
    "\n",
    "        return result\n",
    "    \n",
    "\n",
    "\n",
    "    def prepare_data_for_modeling(self, currency, number_of_files=5, nrows='7d', resample_freq='5min', start_time=None, end_time=None, freq='1min'):\n",
    "        if isinstance(nrows, str):\n",
    "            nrows = self.duration_to_rows(nrows, freq=resample_freq)\n",
    "            print(f\"Converted nrows='{nrows}' based on duration for frequency='{resample_freq}'\")\n",
    "\n",
    "        files = self.filter_by_currency(currency)\n",
    "        raw_data = self.load_latest_data(files, number_of_files, nrows)\n",
    "        for pair_name, df in raw_data.items():\n",
    "            self.check_missing_data_in_range(\n",
    "                df=df,\n",
    "                start_time=start_time,\n",
    "                end_time=end_time,\n",
    "                freq=freq,\n",
    "                pair_name=pair_name\n",
    "            )\n",
    "        \n",
    "        prepared = {}\n",
    "        for symbol, df in raw_data.items():\n",
    "            print(f\"Processing {symbol} with {len(df)} rows.\")\n",
    "            cleaned = self.validate_and_clean(df, freq='1min')  # always validate at 1min base\n",
    "            if cleaned is None:\n",
    "                print(f\"Skipping {symbol}: too many missing values.\")\n",
    "                continue\n",
    "            resampled = self.resample_data(cleaned, freq=resample_freq)\n",
    "            if len(resampled) < 100:\n",
    "                print(f\"Skipping {symbol}: not enough data after resampling.\")\n",
    "                continue\n",
    "            train, val, test = self.split_data(resampled)\n",
    "            prepared[symbol] = {\n",
    "                'train': train,\n",
    "                'val': val,\n",
    "                'test': test\n",
    "            }\n",
    "\n",
    "        return prepared\n",
    "\n",
    "    def duration_to_rows(self, duration_str: str, freq: str) -> int:\n",
    "        \"\"\"\n",
    "        Converts a time duration like '1d', '2w', '3mo' into a number of rows based on frequency.\n",
    "        freq: a pandas-compatible frequency string like '1min', '5min', '1H', '1D'\n",
    "        \"\"\"\n",
    "        freq_offset = pd.tseries.frequencies.to_offset(freq)\n",
    "        freq_timedelta = freq_offset.delta or pd.to_timedelta(freq_offset)\n",
    "\n",
    "        duration_str = duration_str.strip().lower()\n",
    "        duration_map = {\n",
    "            'd': 'D',\n",
    "            'w': 'W',\n",
    "            'mo': 'M',\n",
    "            'y': 'Y'\n",
    "        }\n",
    "\n",
    "        match = re.match(r'(\\d+)([a-z]+)', duration_str)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Invalid duration format: {duration_str}\")\n",
    "\n",
    "        num, unit = int(match.group(1)), match.group(2)\n",
    "        if unit not in duration_map:\n",
    "            raise ValueError(f\"Unsupported duration unit: {unit}\")\n",
    "\n",
    "        now = pd.Timestamp.utcnow()\n",
    "        if unit == 'mo':\n",
    "            start = now - pd.DateOffset(months=num)\n",
    "        elif unit == 'y':\n",
    "            start = now - pd.DateOffset(years=num)\n",
    "        elif unit == 'w':\n",
    "            start = now - pd.Timedelta(weeks=num)\n",
    "        else:  # 'd'\n",
    "            start = now - pd.Timedelta(days=num)\n",
    "\n",
    "        delta = now - start\n",
    "        row_count = int(delta.total_seconds() // freq_timedelta.total_seconds())\n",
    "        return row_count\n",
    "\n",
    "        \n",
    "\n",
    "        # return consistent_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping tonusd.csv: not enough data.\n",
      "Skipping galausd.csv: not enough data.\n",
      "Skipping velo-usd.csv: not enough data.\n",
      "Skipping waxusd.csv: not enough data.\n",
      "Skipping planetsusd.csv: not enough data.\n",
      "Skipping boousd.csv: not enough data.\n",
      "Skipping astusd.csv: not enough data.\n",
      "Skipping briseusd.csv: not enough data.\n",
      "Skipping exousd.csv: not enough data.\n",
      "Skipping suku-usd.csv: not enough data.\n",
      "Skipping band-usd.csv: not enough data.\n",
      "Skipping chzusd.csv: not enough data.\n",
      "Skipping boson-usd.csv: not enough data.\n",
      "Skipping arbusd.csv: not enough data.\n",
      "Skipping testalgotestusd.csv: not enough data.\n",
      "Skipping xautusd.csv: not enough data.\n",
      "Skipping genusd.csv: not enough data.\n",
      "Skipping reefusd.csv: not enough data.\n",
      "Skipping luxousd.csv: not enough data.\n",
      "Skipping pngusd.csv: not enough data.\n",
      "Skipping gmtusd.csv: not enough data.\n",
      "Skipping laiusd.csv: not enough data.\n",
      "Skipping sidus_usd.csv: not enough data.\n",
      "Skipping dora_usd.csv: not enough data.\n",
      "Skipping swmusd.csv: not enough data.\n",
      "Skipping polisusd.csv: not enough data.\n",
      "Skipping gxtusd.csv: not enough data.\n",
      "Skipping maticusd.csv: not enough data.\n",
      "Skipping dogeusd.csv: not enough data.\n",
      "Skipping chex-usd.csv: not enough data.\n",
      "Skipping wncg_usd.csv: not enough data.\n",
      "Skipping flokiusd.csv: not enough data.\n",
      "Skipping testltctestusd.csv: not enough data.\n",
      "Skipping gala_usd.csv: not enough data.\n",
      "Skipping matic_usd.csv: not enough data.\n",
      "Skipping spellusd.csv: not enough data.\n",
      "Skipping suiusd.csv: not enough data.\n",
      "Skipping oneusd.csv: not enough data.\n",
      "Skipping egldusd.csv: not enough data.\n",
      "Skipping reef-usd.csv: not enough data.\n",
      "Skipping zbtusd.csv: not enough data.\n",
      "Skipping waves_usd.csv: not enough data.\n",
      "Skipping ocean-usd.csv: not enough data.\n",
      "Skipping okbusd.csv: not enough data.\n",
      "Skipping antusd.csv: not enough data.\n",
      "Skipping 1inch-usd.csv: not enough data.\n",
      "Skipping woousd.csv: not enough data.\n",
      "Skipping luna_usd.csv: not enough data.\n",
      "Skipping testxtztestusd.csv: not enough data.\n",
      "Skipping senateusd.csv: not enough data.\n",
      "Skipping planets-usd.csv: not enough data.\n",
      "Skipping polc_usd.csv: not enough data.\n",
      "Skipping ctxusd.csv: not enough data.\n",
      "Skipping gptusd.csv: not enough data.\n",
      "Skipping boba_usd.csv: not enough data.\n",
      "Skipping bchabc-usd.csv: not enough data.\n",
      "Skipping orsusd.csv: not enough data.\n",
      "Skipping sand_usd.csv: not enough data.\n",
      "Skipping veeusd.csv: not enough data.\n",
      "Skipping bchnusd.csv: not enough data.\n",
      "Skipping necusd.csv: not enough data.\n",
      "Skipping qtfusd.csv: not enough data.\n",
      "Skipping chsb-usd.csv: not enough data.\n",
      "Skipping htxusd.csv: not enough data.\n",
      "Skipping vrausd.csv: not enough data.\n",
      "Skipping shftusd.csv: not enough data.\n",
      "Skipping bmiusd.csv: not enough data.\n",
      "Skipping luxo_usd.csv: not enough data.\n",
      "Skipping bchabcusd.csv: not enough data.\n",
      "Skipping spell_usd.csv: not enough data.\n",
      "Skipping thetausd.csv: not enough data.\n",
      "Skipping qrdo_usd.csv: not enough data.\n",
      "Skipping testavaxtestusd.csv: not enough data.\n",
      "Skipping tlos_usd.csv: not enough data.\n",
      "Skipping sxxusd.csv: not enough data.\n",
      "Skipping ocean_usd.csv: not enough data.\n",
      "Skipping waves-usd.csv: not enough data.\n",
      "Skipping reef_usd.csv: not enough data.\n",
      "Skipping testmatictestusd.csv: not enough data.\n",
      "Skipping rrbusd.csv: not enough data.\n",
      "Skipping nomusd.csv: not enough data.\n",
      "Skipping planets_usd.csv: not enough data.\n",
      "Skipping velousd.csv: not enough data.\n",
      "Skipping tenet_usd.csv: not enough data.\n",
      "Skipping luna-usd.csv: not enough data.\n",
      "Skipping testdogetestusd.csv: not enough data.\n",
      "Skipping shib_usd.csv: not enough data.\n",
      "Skipping dtxusd.csv: not enough data.\n",
      "Skipping 1inch_usd.csv: not enough data.\n",
      "Skipping tlosusd.csv: not enough data.\n",
      "Skipping chsb_usd.csv: not enough data.\n",
      "Skipping wminimausd.csv: not enough data.\n",
      "Skipping ethwusd.csv: not enough data.\n",
      "Skipping eosdt-usd.csv: not enough data.\n",
      "Skipping grtusd.csv: not enough data.\n",
      "Skipping bchabc_usd.csv: not enough data.\n",
      "Skipping senate_usd.csv: not enough data.\n",
      "Skipping uopusd.csv: not enough data.\n",
      "Skipping hecusd.csv: not enough data.\n",
      "Skipping sushiusd.csv: not enough data.\n",
      "Skipping zilusd.csv: not enough data.\n",
      "Skipping odeusd.csv: not enough data.\n",
      "Skipping prmx_usd.csv: not enough data.\n",
      "Skipping suku_usd.csv: not enough data.\n",
      "Skipping band_usd.csv: not enough data.\n",
      "Skipping testethtestusd.csv: not enough data.\n",
      "Skipping nexousd.csv: not enough data.\n",
      "Skipping wminima_usd.csv: not enough data.\n",
      "Skipping testapttestusd.csv: not enough data.\n",
      "Skipping velo_usd.csv: not enough data.\n",
      "Skipping gstusd.csv: not enough data.\n",
      "Skipping sidususd.csv: not enough data.\n",
      "Skipping eth2xusd.csv: not enough data.\n",
      "Skipping prmxusd.csv: not enough data.\n",
      "Skipping boson_usd.csv: not enough data.\n",
      "Skipping testxauttestusd.csv: not enough data.\n",
      "Skipping nutusd.csv: not enough data.\n",
      "Skipping dcrusd.csv: not enough data.\n",
      "Skipping chex_usd.csv: not enough data.\n",
      "Skipping kaiusd.csv: not enough data.\n",
      "Skipping lunausd.csv: not enough data.\n",
      "Skipping albt-usd.csv: not enough data.\n",
      "Skipping polcusd.csv: not enough data.\n",
      "Skipping iosusd.csv: not enough data.\n",
      "Skipping bandusd.csv: not enough data.\n",
      "Skipping gotusd.csv: not enough data.\n",
      "Skipping pepe_usd.csv: not enough data.\n",
      "Skipping nearusd.csv: not enough data.\n",
      "Skipping poausd.csv: not enough data.\n",
      "Skipping testadatestusd.csv: not enough data.\n",
      "Skipping flrusd.csv: not enough data.\n",
      "Skipping dvfusd.csv: not enough data.\n",
      "Skipping dorausd.csv: not enough data.\n",
      "Skipping near-usd.csv: not enough data.\n",
      "Skipping aave-usd.csv: not enough data.\n",
      "Skipping sushi-usd.csv: not enough data.\n",
      "Skipping forthusd.csv: not enough data.\n",
      "Skipping terraustusd.csv: not enough data.\n",
      "Skipping chexusd.csv: not enough data.\n",
      "Skipping forth-usd.csv: not enough data.\n",
      "Skipping egld_usd.csv: not enough data.\n",
      "Skipping luna2usd.csv: not enough data.\n",
      "Skipping apenft_usd.csv: not enough data.\n",
      "Skipping testdottestusd.csv: not enough data.\n",
      "Skipping karate_usd.csv: not enough data.\n",
      "Skipping ringx-usd.csv: not enough data.\n",
      "Skipping xaut-usd.csv: not enough data.\n",
      "Skipping testneartestusd.csv: not enough data.\n",
      "Skipping eth2x_usd.csv: not enough data.\n",
      "Skipping rcnusd.csv: not enough data.\n",
      "Skipping omnusd.csv: not enough data.\n",
      "Skipping oceanusd.csv: not enough data.\n",
      "Skipping opxusd.csv: not enough data.\n",
      "Skipping trade_usd.csv: not enough data.\n",
      "Skipping bntusd.csv: not enough data.\n",
      "Skipping doge-usd.csv: not enough data.\n",
      "Skipping wprusd.csv: not enough data.\n",
      "Skipping xcnusd.csv: not enough data.\n",
      "Skipping whbt_usd.csv: not enough data.\n",
      "Skipping theta_usd.csv: not enough data.\n",
      "Skipping mxnt_usd.csv: not enough data.\n",
      "Skipping b21x-usd.csv: not enough data.\n",
      "Skipping nexo_usd.csv: not enough data.\n",
      "Skipping qrdousd.csv: not enough data.\n",
      "Skipping nxrausd.csv: not enough data.\n",
      "Skipping comp_usd.csv: not enough data.\n",
      "Skipping pasusd.csv: not enough data.\n",
      "Skipping sngusd.csv: not enough data.\n",
      "Skipping sweatusd.csv: not enough data.\n",
      "Skipping wtcusd.csv: not enough data.\n",
      "Skipping dusk-usd.csv: not enough data.\n",
      "Skipping enjusd.csv: not enough data.\n",
      "Skipping boxusd.csv: not enough data.\n",
      "Skipping yggusd.csv: not enough data.\n",
      "Skipping convusd.csv: not enough data.\n",
      "Skipping linkusd.csv: not enough data.\n",
      "Skipping rifusd.csv: not enough data.\n",
      "Skipping bobausd.csv: not enough data.\n",
      "Skipping compusd.csv: not enough data.\n",
      "Skipping treeb_usd.csv: not enough data.\n",
      "Skipping ncausd.csv: not enough data.\n",
      "Skipping exrd-usd.csv: not enough data.\n",
      "Skipping testbtctestusd.csv: not enough data.\n",
      "Skipping bchn-usd.csv: not enough data.\n",
      "Skipping wavesusd.csv: not enough data.\n",
      "Skipping xcad_usd.csv: not enough data.\n",
      "Skipping link-usd.csv: not enough data.\n",
      "Skipping ldousd.csv: not enough data.\n",
      "Skipping hotusd.csv: not enough data.\n",
      "Skipping blurusd.csv: not enough data.\n",
      "Skipping atlasusd.csv: not enough data.\n",
      "Skipping best_usd.csv: not enough data.\n",
      "Skipping btse-usd.csv: not enough data.\n",
      "Skipping eususd.csv: not enough data.\n",
      "Skipping atlas_usd.csv: not enough data.\n",
      "Skipping avax-usd.csv: not enough data.\n",
      "Skipping shibusd.csv: not enough data.\n",
      "Skipping bgbusd.csv: not enough data.\n",
      "Skipping wildusd.csv: not enough data.\n",
      "Skipping hezusd.csv: not enough data.\n",
      "Skipping 1inchusd.csv: not enough data.\n",
      "Skipping terraust-usd.csv: not enough data.\n",
      "Skipping bosonusd.csv: not enough data.\n",
      "Skipping dusk_usd.csv: not enough data.\n",
      "Skipping luna2_usd.csv: not enough data.\n",
      "Skipping blur_usd.csv: not enough data.\n",
      "Skipping bchn_usd.csv: not enough data.\n",
      "Skipping wncgusd.csv: not enough data.\n",
      "Skipping tknusd.csv: not enough data.\n",
      "Skipping ognusd.csv: not enough data.\n",
      "Skipping treebusd.csv: not enough data.\n",
      "Skipping polis_usd.csv: not enough data.\n",
      "Skipping best-usd.csv: not enough data.\n",
      "Skipping dapp-usd.csv: not enough data.\n",
      "Skipping btse_usd.csv: not enough data.\n",
      "Skipping gocusd.csv: not enough data.\n",
      "Skipping mxntusd.csv: not enough data.\n",
      "Skipping manusd.csv: not enough data.\n",
      "Skipping testsoltestusd.csv: not enough data.\n",
      "Skipping mtnusd.csv: not enough data.\n",
      "Skipping link_usd.csv: not enough data.\n",
      "Skipping avax_usd.csv: not enough data.\n",
      "Skipping dtausd.csv: not enough data.\n",
      "Skipping shft_usd.csv: not enough data.\n",
      "Skipping drnusd.csv: not enough data.\n",
      "Skipping sandusd.csv: not enough data.\n",
      "Skipping testeostestusd.csv: not enough data.\n",
      "Skipping ethw_usd.csv: not enough data.\n",
      "Skipping bestusd.csv: not enough data.\n",
      "Skipping btseusd.csv: not enough data.\n",
      "Skipping forth_usd.csv: not enough data.\n",
      "Skipping egld-usd.csv: not enough data.\n",
      "Skipping seiusd.csv: not enough data.\n",
      "Skipping apenftusd.csv: not enough data.\n",
      "Skipping triusd.csv: not enough data.\n",
      "Skipping b2musd.csv: not enough data.\n",
      "Skipping sushi_usd.csv: not enough data.\n",
      "Skipping aave_usd.csv: not enough data.\n",
      "Skipping wild_usd.csv: not enough data.\n",
      "Skipping dgxusd.csv: not enough data.\n",
      "Skipping near_usd.csv: not enough data.\n",
      "Skipping uskusd.csv: not enough data.\n",
      "Skipping brise_usd.csv: not enough data.\n",
      "Skipping eth2x-usd.csv: not enough data.\n",
      "Skipping xaut_usd.csv: not enough data.\n",
      "Skipping chsbusd.csv: not enough data.\n",
      "Skipping tradeusd.csv: not enough data.\n",
      "Skipping requsd.csv: not enough data.\n",
      "Skipping srmusd.csv: not enough data.\n",
      "Skipping onlusd.csv: not enough data.\n",
      "Skipping sweat_usd.csv: not enough data.\n",
      "Skipping xcadusd.csv: not enough data.\n",
      "Skipping jasmy_usd.csv: not enough data.\n",
      "Skipping doge_usd.csv: not enough data.\n",
      "Skipping cndusd.csv: not enough data.\n",
      "Skipping testfiltestusd.csv: not enough data.\n",
      "Skipping funusd.csv: not enough data.\n",
      "Skipping sukuusd.csv: not enough data.\n",
      "Skipping nexo-usd.csv: not enough data.\n",
      "Skipping jasmyusd.csv: not enough data.\n",
      "Skipping comp-usd.csv: not enough data.\n",
      "Skipping fttusd.csv: not enough data.\n",
      "Skipping bmnusd.csv: not enough data.\n",
      "Skipping aaveusd.csv: not enough data.\n",
      "Skipping duskusd.csv: not enough data.\n",
      "Skipping floki_usd.csv: not enough data.\n",
      "Skipping nxra_usd.csv: not enough data.\n",
      "Skipping avtusd.csv: not enough data.\n",
      "Skipping avaxusd.csv: not enough data.\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/Users/macsnaxx/Personal Projects/Crypto_Arbitrage/crypto_data_2020-2024'\n",
    "manager = FileReader(data_dir)\n",
    "data = manager.load_latest_data(selected_files=manager.filter_by_currency('usd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# updated pythonic script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Iterable\n",
    "import re\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FileMeta:\n",
    "    pair: str\n",
    "    currency: Optional[str]\n",
    "    path: Path\n",
    "\n",
    "    @property\n",
    "    def key(self) -> str:\n",
    "        return f\"{self.pair}/{self.currency or 'base'}\"\n",
    "\n",
    "class FilenameParser:\n",
    "    _pattern = re.compile(r\"^(?P<pair>[a-z0-9]+?)(?P<quote>usd|usdt|btc|...)?$\", re.I)\n",
    "\n",
    "    def parse(self, path: Path) -> Optional[FileMeta]:\n",
    "        m = self._pattern.fullmatch(path.stem)\n",
    "        if not m:\n",
    "            return None\n",
    "        return FileMeta(pair=m.group('pair').lower(),\n",
    "                        currency=(m.group('quote') or '').lower() or None,\n",
    "                        path=path)\n",
    "\n",
    "class FileScanner:\n",
    "    def __init__(self, directory: Path):\n",
    "        self.directory = directory\n",
    "\n",
    "    def scan(self) -> list[FileMeta]:\n",
    "        files = []\n",
    "        for path in self.directory.iterdir():\n",
    "            if path.suffix.lower() == \".csv\" and path.is_file():\n",
    "                meta = FilenameParser().parse(path)\n",
    "                if meta:\n",
    "                    files.append(meta)\n",
    "        return files\n",
    "\n",
    "class CsvLoader:\n",
    "    def __init__(self, parse_dates: list[str] = ['time'], date_unit='ms', index_col='time'):\n",
    "        self.parse_dates = parse_dates\n",
    "        self.date_unit = date_unit\n",
    "        self.index_col = index_col\n",
    "\n",
    "    def load(self, metas: Iterable[FileMeta], nrows: Optional[int] = None) -> dict[str, pd.DataFrame]:\n",
    "        out = {}\n",
    "        for meta in metas:\n",
    "            df = pd.read_csv(meta.path,\n",
    "                             nrows=nrows,\n",
    "                             parse_dates=self.parse_dates,\n",
    "                             date_unit=self.date_unit,\n",
    "                             index_col=self.index_col)\n",
    "            out[meta.key] = df\n",
    "        return out\n",
    "\n",
    "class DataManager:\n",
    "    def __init__(self, data_dir: str):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self._index: list[FileMeta] = None\n",
    "        self.loader = CsvLoader()\n",
    "\n",
    "    @property\n",
    "    def index(self) -> list[FileMeta]:\n",
    "        if self._index is None:\n",
    "            self._index = FileScanner(self.data_dir).scan()\n",
    "        return self._index\n",
    "\n",
    "    def filter(self, currency: Optional[str] = None) -> list[FileMeta]:\n",
    "        if currency:\n",
    "            c = currency.lower()\n",
    "            return [f for f in self.index if f.currency == c]\n",
    "        return list(self.index)\n",
    "\n",
    "    def load(self, currency: Optional[str] = None,\n",
    "             number_of_files: Optional[int] = None,\n",
    "             nrows: Optional[int] = None) -> dict[str, pd.DataFrame]:\n",
    "        selected = self.filter(currency)\n",
    "        if number_of_files:\n",
    "            selected = selected[:number_of_files]\n",
    "        if not selected:\n",
    "            raise ValueError(f\"No files selected for currency={currency}\")\n",
    "        return self.loader.load(selected, nrows=nrows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
